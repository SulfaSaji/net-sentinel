{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72392a40-40c6-4813-b5b4-a377fc830381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # For evaluation\n",
    "import joblib # For saving the trained model\n",
    "import os # For path manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7acf1a1-d172-49e1-ba65-307405f9b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = R\"C:\\Users\\seena\\Downloads\\Network-Intrusion-Detection-System\\processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424a2af-fd14-4dbf-a728-747bc2f63341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_FILENAME = 'final_netshield_cleaned_scaled_dataset.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e818812e-de90-475d-9b29-8ff8df34f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT_DIR, DATASET_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d6a1d5-53cf-41a5-ac14-ccb8c5ff026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'trained_model')\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, 'network_intrusion_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ae18da-1237-479b-bcbc-c89358fce3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Loading the Cleaned Dataset ---\n",
      "Attempting to load dataset from: C:\\Users\\seena\\Downloads\\Network-Intrusion-Detection-System\\processed_data\\final_netshield_cleaned_scaled_dataset.parquet\n",
      "Dataset loaded successfully from: C:\\Users\\seena\\Downloads\\Network-Intrusion-Detection-System\\processed_data\\final_netshield_cleaned_scaled_dataset.parquet\n",
      "Dataset shape: (2596603, 79)\n",
      "First 5 rows of the dataset:\n",
      "   protocol  flow_duration  total_fwd_packets  total_backward_packets  \\\n",
      "0         6      -0.485030          -0.011998               -0.010310   \n",
      "1         6      -0.485033          -0.011998               -0.009399   \n",
      "2        17      -0.482243          -0.011998               -0.010310   \n",
      "3        17      -0.484001          -0.011998               -0.010310   \n",
      "4         0       2.619704           0.164207               -0.011221   \n",
      "\n",
      "   fwd_packets_length_total  bwd_packets_length_total  fwd_packet_length_max  \\\n",
      "0                 -0.062106                 -0.007681              -0.330554   \n",
      "1                 -0.062106                 -0.007681              -0.330554   \n",
      "2                 -0.057573                 -0.007662              -0.266827   \n",
      "3                 -0.057573                 -0.007662              -0.266827   \n",
      "4                 -0.062106                 -0.007681              -0.330554   \n",
      "\n",
      "   fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  ...  \\\n",
      "0              -0.348984               -0.350344              -0.277703  ...   \n",
      "1              -0.348984               -0.350344              -0.277703  ...   \n",
      "2               0.439501               -0.101253              -0.277703  ...   \n",
      "3               0.439501               -0.101253              -0.277703  ...   \n",
      "4              -0.348984               -0.350344              -0.277703  ...   \n",
      "\n",
      "   active_mean  active_std  active_max  active_min  idle_mean  idle_std  \\\n",
      "0    -0.140546   -0.116329   -0.165416   -0.112365  -0.382225 -0.118763   \n",
      "1    -0.140546   -0.116329   -0.165416   -0.112365  -0.382225 -0.118763   \n",
      "2    -0.140546   -0.116329   -0.165416   -0.112365  -0.382225 -0.118763   \n",
      "3    -0.140546   -0.116329   -0.165416   -0.112365  -0.382225 -0.118763   \n",
      "4     2.412448    4.852934    4.190187   -0.112209  -0.000818  0.427598   \n",
      "\n",
      "   idle_max  idle_min   label  label_encoded  \n",
      "0 -0.387751 -0.367751  NORMAL              8  \n",
      "1 -0.387751 -0.367751  NORMAL              8  \n",
      "2 -0.387751 -0.367751  NORMAL              8  \n",
      "3 -0.387751 -0.367751  NORMAL              8  \n",
      "4  0.143864 -0.135487  NORMAL              8  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "\n",
      "Dataset information (data types, non-null values):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2596603 entries, 0 to 2596602\n",
      "Data columns (total 79 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   protocol                  int8   \n",
      " 1   flow_duration             float64\n",
      " 2   total_fwd_packets         float64\n",
      " 3   total_backward_packets    float64\n",
      " 4   fwd_packets_length_total  float64\n",
      " 5   bwd_packets_length_total  float64\n",
      " 6   fwd_packet_length_max     float64\n",
      " 7   fwd_packet_length_min     float64\n",
      " 8   fwd_packet_length_mean    float64\n",
      " 9   fwd_packet_length_std     float64\n",
      " 10  bwd_packet_length_max     float64\n",
      " 11  bwd_packet_length_min     float64\n",
      " 12  bwd_packet_length_mean    float64\n",
      " 13  bwd_packet_length_std     float64\n",
      " 14  flow_bytess               float64\n",
      " 15  flow_packetss             float64\n",
      " 16  flow_iat_mean             float64\n",
      " 17  flow_iat_std              float64\n",
      " 18  flow_iat_max              float64\n",
      " 19  flow_iat_min              float64\n",
      " 20  fwd_iat_total             float64\n",
      " 21  fwd_iat_mean              float64\n",
      " 22  fwd_iat_std               float64\n",
      " 23  fwd_iat_max               float64\n",
      " 24  fwd_iat_min               float64\n",
      " 25  bwd_iat_total             float64\n",
      " 26  bwd_iat_mean              float64\n",
      " 27  bwd_iat_std               float64\n",
      " 28  bwd_iat_max               float64\n",
      " 29  bwd_iat_min               float64\n",
      " 30  fwd_psh_flags             int8   \n",
      " 31  bwd_psh_flags             int8   \n",
      " 32  fwd_urg_flags             int8   \n",
      " 33  bwd_urg_flags             int8   \n",
      " 34  fwd_header_length         float64\n",
      " 35  bwd_header_length         float64\n",
      " 36  fwd_packetss              float64\n",
      " 37  bwd_packetss              float64\n",
      " 38  packet_length_min         float64\n",
      " 39  packet_length_max         float64\n",
      " 40  packet_length_mean        float64\n",
      " 41  packet_length_std         float64\n",
      " 42  packet_length_variance    float64\n",
      " 43  fin_flag_count            int8   \n",
      " 44  syn_flag_count            int8   \n",
      " 45  rst_flag_count            int8   \n",
      " 46  psh_flag_count            int8   \n",
      " 47  ack_flag_count            int8   \n",
      " 48  urg_flag_count            int8   \n",
      " 49  cwe_flag_count            int8   \n",
      " 50  ece_flag_count            int8   \n",
      " 51  downup_ratio              int16  \n",
      " 52  avg_packet_size           float64\n",
      " 53  avg_fwd_segment_size      float64\n",
      " 54  avg_bwd_segment_size      float64\n",
      " 55  fwd_avg_bytesbulk         int8   \n",
      " 56  fwd_avg_packetsbulk       int8   \n",
      " 57  fwd_avg_bulk_rate         int8   \n",
      " 58  bwd_avg_bytesbulk         int8   \n",
      " 59  bwd_avg_packetsbulk       int8   \n",
      " 60  bwd_avg_bulk_rate         int8   \n",
      " 61  subflow_fwd_packets       float64\n",
      " 62  subflow_fwd_bytes         float64\n",
      " 63  subflow_bwd_packets       float64\n",
      " 64  subflow_bwd_bytes         float64\n",
      " 65  init_fwd_win_bytes        float64\n",
      " 66  init_bwd_win_bytes        float64\n",
      " 67  fwd_act_data_packets      float64\n",
      " 68  fwd_seg_size_min          float64\n",
      " 69  active_mean               float64\n",
      " 70  active_std                float64\n",
      " 71  active_max                float64\n",
      " 72  active_min                float64\n",
      " 73  idle_mean                 float64\n",
      " 74  idle_std                  float64\n",
      " 75  idle_max                  float64\n",
      " 76  idle_min                  float64\n",
      " 77  label                     object \n",
      " 78  label_encoded             int64  \n",
      "dtypes: float64(57), int16(1), int64(1), int8(19), object(1)\n",
      "memory usage: 1.2+ GB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---  Loading the Cleaned Dataset ---\")\n",
    "print(f\"Attempting to load dataset from: {DATASET_PATH}\")\n",
    "try:\n",
    "    # Load the Parquet file into a pandas DataFrame using pyarrow engine\n",
    "    df = pd.read_parquet(DATASET_PATH, engine=\"pyarrow\")\n",
    "    print(f\"Dataset loaded successfully from: {DATASET_PATH}\")\n",
    "    print(f\"Dataset shape: {df.shape}\") # Shows (number of rows, number of columns)\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(df.head()) # Displays the first 5 rows of your data\n",
    "\n",
    "    print(\"\\nDataset information (data types, non-null values):\")\n",
    "    df.info() # Provides a summary of your dataset's columns and data types\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset file not found at: {DATASET_PATH}\")\n",
    "    print(\"Please double-check that the file name and the 'PROJECT_ROOT_DIR' are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected problem occurred while loading the dataset:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # Prints full technical details of the error\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b13da6-7a05-4931-bf74-e61a78e6f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings defined: encoded_to_label and NORMAL_ENCODED_VALUE.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(df['label'])\n",
    "\n",
    "encoded_to_label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "NORMAL_ENCODED_VALUE = next(key for key, value in encoded_to_label.items() if value == 'NORMAL')\n",
    "\n",
    "print(\"Mappings defined: encoded_to_label and NORMAL_ENCODED_VALUE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a84510-5da9-4795-83d4-51a3e723a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Dividing Data into Training and Test Sets ---\n",
      "\n",
      "Shape of Features (X):(2596603, 77)\n",
      "Shape of Target (y):(2596603,)\n",
      "\n",
      "First 5 columns of X (Features):\n",
      "   protocol  flow_duration  total_fwd_packets  total_backward_packets  \\\n",
      "0         6      -0.485030          -0.011998               -0.010310   \n",
      "1         6      -0.485033          -0.011998               -0.009399   \n",
      "2        17      -0.482243          -0.011998               -0.010310   \n",
      "3        17      -0.484001          -0.011998               -0.010310   \n",
      "4         0       2.619704           0.164207               -0.011221   \n",
      "\n",
      "   fwd_packets_length_total  \n",
      "0                 -0.062106  \n",
      "1                 -0.062106  \n",
      "2                 -0.057573  \n",
      "3                 -0.057573  \n",
      "4                 -0.062106  \n"
     ]
    }
   ],
   "source": [
    "print(\"---  Dividing Data into Training and Test Sets ---\")\n",
    "all_columns=df.columns.tolist()\n",
    "features_to_exclude=['label','label_encoded']\n",
    "X_columns=[col for col in all_columns if col not in features_to_exclude]\n",
    "X=df[X_columns]\n",
    "y=df['label_encoded']\n",
    "print(f\"\\nShape of Features (X):{X.shape}\")\n",
    "print(f\"Shape of Target (y):{y.shape}\")\n",
    "print(\"\\nFirst 5 columns of X (Features):\")\n",
    "print(X.iloc[:, :5].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "960670fb-6edf-4f66-8655-8ffc2a47882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Train-Test Split ---\n",
      "Data split into training and test sets successfully!\n",
      "X_train shape: (2077282, 77)\n",
      "y_train shape: (2077282,)\n",
      "X_test shape: (519321, 77)\n",
      "y_test shape: (519321,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n--- Performing Train-Test Split ---\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# test_size=0.2 means 20% of the data will be used for testing\n",
    "# random_state=42 ensures reproducibility\n",
    "# stratify=y ensures that the proportion of classes is the same in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split into training and test sets successfully!\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a744645-961f-421b-9c66-470fb1bcd459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NORMAL                        2261548\n",
       "DOS HULK                       172846\n",
       "DDOS                           128014\n",
       "DOS GOLDENEYE                   10286\n",
       "FTP-PATATOR                      5931\n",
       "DOS SLOWLORIS                    5385\n",
       "DOS SLOWHTTPTEST                 5228\n",
       "SSH-PATATOR                      3219\n",
       "PORTSCAN                         1956\n",
       "WEB ATTACK � BRUTE FORCE         1470\n",
       "WEB ATTACK � XSS                  652\n",
       "INFILTRATION                       36\n",
       "WEB ATTACK � SQL INJECTION         21\n",
       "HEARTBLEED                         11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() # Replace 'label' with your actual target column name if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d978b6d-4972-4d9d-947a-e2bfa517b34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of X_train: {type(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafe18f1-bffc-4c80-81f9-ab0a9b98c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\seena\\downloads\\network-intrusion-detection-system\\venv\\lib\\site-packages (from imbalanced-learn) (2.3.0)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\seena\\downloads\\network-intrusion-detection-system\\venv\\lib\\site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\seena\\downloads\\network-intrusion-detection-system\\venv\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\seena\\downloads\\network-intrusion-detection-system\\venv\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\seena\\downloads\\network-intrusion-detection-system\\venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n",
      "1.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# In your Jupyter Notebook cell, while newvenv kernel is active:\n",
    "\n",
    "# 1. Ensure imbalanced-learn is at the desired version\n",
    "#    (It already seems to be 0.13.0, but this doesn't hurt)\n",
    "!pip install imbalanced-learn\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92190f14-a6e9-4c52-b9cd-e6545bde9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed SMOTE sampling strategy:\n",
      "{0: 102411, 1: 50000, 2: 138277, 3: 50000, 4: 50000, 5: 50000, 6: 50000, 7: 50000, 8: 1809238, 9: 50000, 10: 50000, 11: 50000, 12: 50000, 13: 50000}\n",
      "\n",
      "--- Applying SMOTE with custom sampling_strategy... ---\n",
      "\n",
      "--- Class distribution AFTER SMOTE with custom strategy ---\n",
      "label_encoded\n",
      "0      102411\n",
      "1       50000\n",
      "2      138277\n",
      "3       50000\n",
      "4       50000\n",
      "5       50000\n",
      "6       50000\n",
      "7       50000\n",
      "8     1809238\n",
      "9       50000\n",
      "10      50000\n",
      "11      50000\n",
      "12      50000\n",
      "13      50000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# First, get the current class distribution of your training data\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "\n",
    "# Define your custom sampling strategy\n",
    "# We want to oversample all minority classes, but not necessarily to the size of 'NORMAL'.\n",
    "# Let's target a size significantly larger than current minority classes,\n",
    "# for example, let's try to bring them all up to 100,000 samples each, or\n",
    "# roughly the size of your largest non-NORMAL class (DOS HULK ~138k).\n",
    "# We'll set a reasonable target, e.g., 50,000 to 100,000 samples for each minority class.\n",
    "\n",
    "# Example: Set a target count for minority classes.\n",
    "# Let's try to bring all minority classes to, say, 50,000 samples.\n",
    "# (Adjust this number based on your memory and desired balance)\n",
    "target_minority_size = 50000\n",
    "\n",
    "# Create the sampling_strategy dictionary\n",
    "sampling_strategy = {}\n",
    "for cls, count in class_counts.items():\n",
    "    if cls != NORMAL_ENCODED_VALUE: # For all non-NORMAL classes\n",
    "        # Ensure we don't try to resample a class smaller than its current count\n",
    "        sampling_strategy[cls] = max(count, target_minority_size) # Take max of current or target size\n",
    "    else:\n",
    "        # Keep the majority class as is\n",
    "        sampling_strategy[cls] = count # Or simply exclude it from the strategy dictionary\n",
    "\n",
    "print(\"Proposed SMOTE sampling strategy:\")\n",
    "print(sampling_strategy)\n",
    "\n",
    "# Re-initialize SMOTE with the custom sampling_strategy\n",
    "smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=sampling_strategy)\n",
    "\n",
    "# Now, try to apply SMOTE again\n",
    "print(\"\\n--- Applying SMOTE with custom sampling_strategy... ---\")\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Class distribution AFTER SMOTE with custom strategy ---\")\n",
    "print(pd.Series(y_train_resampled).value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f81750-55ed-406c-b887-f6377a5b2edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RandomForestClassifier on SMOTE-resampled data ---\n",
      "Training data shape (resampled): (2599926, 77)\n",
      "Training target shape (resampled): (2599926,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier training on SMOTE-resampled data complete!\n",
      "\n",
      "--- Evaluating the Model (trained with SMOTE) on the ORIGINAL Test Set (Multi-class) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set (after SMOTE training): 0.9990\n",
      "\n",
      "Classification Report (Multi-class, after SMOTE training):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                      DDOS       1.00      1.00      1.00     25603\n",
      "             DOS GOLDENEYE       1.00      1.00      1.00      2057\n",
      "                  DOS HULK       1.00      1.00      1.00     34569\n",
      "          DOS SLOWHTTPTEST       0.95      0.99      0.97      1046\n",
      "             DOS SLOWLORIS       1.00      0.99      1.00      1077\n",
      "               FTP-PATATOR       1.00      1.00      1.00      1186\n",
      "                HEARTBLEED       1.00      1.00      1.00         2\n",
      "              INFILTRATION       1.00      0.71      0.83         7\n",
      "                    NORMAL       1.00      1.00      1.00    452310\n",
      "                  PORTSCAN       0.91      0.95      0.93       391\n",
      "               SSH-PATATOR       1.00      0.98      0.99       644\n",
      "  WEB ATTACK � BRUTE FORCE       0.78      0.77      0.78       294\n",
      "WEB ATTACK � SQL INJECTION       1.00      0.25      0.40         4\n",
      "          WEB ATTACK � XSS       0.50      0.50      0.50       131\n",
      "\n",
      "                  accuracy                           1.00    519321\n",
      "                 macro avg       0.94      0.87      0.88    519321\n",
      "              weighted avg       1.00      1.00      1.00    519321\n",
      "\n",
      "\n",
      "Confusion Matrix (Multi-class, after SMOTE training):\n",
      "[[ 25589      0      1      0      0      0      0      0     13      0\n",
      "       0      0      0      0]\n",
      " [     0   2050      3      2      0      0      0      0      2      0\n",
      "       0      0      0      0]\n",
      " [     2      4  34421      0      0      0      0      0    130     11\n",
      "       0      1      0      0]\n",
      " [     0      2      0   1038      2      0      0      0      4      0\n",
      "       0      0      0      0]\n",
      " [     0      0      0      4   1069      0      0      0      4      0\n",
      "       0      0      0      0]\n",
      " [     0      0      0      0      0   1183      0      0      3      0\n",
      "       0      0      0      0]\n",
      " [     0      0      0      0      0      0      2      0      0      0\n",
      "       0      0      0      0]\n",
      " [     0      0      0      0      0      0      0      5      2      0\n",
      "       0      0      0      0]\n",
      " [     6      3     57     53      0      1      0      0 452167     21\n",
      "       0      0      0      2]\n",
      " [     0      0      3      0      0      0      0      0     15    372\n",
      "       0      1      0      0]\n",
      " [     0      0      0      0      0      0      0      0     12      0\n",
      "     632      0      0      0]\n",
      " [     0      0      0      0      0      0      0      0      5      0\n",
      "       0    227      0     62]\n",
      " [     0      1      0      0      0      0      0      0      1      0\n",
      "       0      1      1      0]\n",
      " [     0      0      0      0      0      0      0      0      2      3\n",
      "       0     61      0     65]]\n",
      "\n",
      "--- Binary (Malicious vs. Not Malicious) Classification Report (after SMOTE training) ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Malicious (0)       1.00      1.00      1.00    452310\n",
      "    Malicious (1)       1.00      1.00      1.00     67011\n",
      "\n",
      "         accuracy                           1.00    519321\n",
      "        macro avg       1.00      1.00      1.00    519321\n",
      "     weighted avg       1.00      1.00      1.00    519321\n",
      "\n",
      "\n",
      "--- Binary Confusion Matrix (after SMOTE training) ---\n",
      "[[452167    143]\n",
      " [   193  66818]]\n",
      "\n",
      "--- Investigating FALSE NEGATIVES (Actual Malicious, Predicted Not Malicious) after SMOTE training ---\n",
      "\n",
      "Total FALSE NEGATIVES (after SMOTE training): 193\n",
      "\n",
      "Distribution of Missed Attack Types (False Negatives) after SMOTE training:\n",
      "DOS HULK                      130\n",
      "PORTSCAN                       15\n",
      "DDOS                           13\n",
      "SSH-PATATOR                    12\n",
      "WEB ATTACK � BRUTE FORCE        5\n",
      "DOS SLOWHTTPTEST                4\n",
      "DOS SLOWLORIS                   4\n",
      "FTP-PATATOR                     3\n",
      "INFILTRATION                    2\n",
      "DOS GOLDENEYE                   2\n",
      "WEB ATTACK � XSS                2\n",
      "WEB ATTACK � SQL INJECTION      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Training RandomForestClassifier on SMOTE-resampled data ---\")\n",
    "\n",
    "\n",
    "rf_model_smote = RandomForestClassifier(\n",
    "    n_estimators=100,        # Number of trees (can be increased for more performance)\n",
    "    random_state=42,         # For reproducibility\n",
    "    n_jobs=-1,               # Use all available CPU cores for faster training\n",
    "    verbose=1                # Show training progress\n",
    ")\n",
    "\n",
    "print(f\"Training data shape (resampled): {X_train_resampled.shape}\")\n",
    "print(f\"Training target shape (resampled): {y_train_resampled.shape}\")\n",
    "\n",
    "rf_model_smote.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"\\nRandomForestClassifier training on SMOTE-resampled data complete!\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluating the Model (trained with SMOTE) on the ORIGINAL Test Set (Multi-class) ---\")\n",
    "\n",
    "y_pred_smote = rf_model_smote.predict(X_test)\n",
    "\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "print(f\"Accuracy on Test Set (after SMOTE training): {accuracy_smote:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Multi-class, after SMOTE training):\")\n",
    "\n",
    "target_names_multi = [encoded_to_label[i] for i in sorted(encoded_to_label.keys())]\n",
    "print(classification_report(y_test, y_pred_smote, target_names=target_names_multi, zero_division=0)) # Added zero_division=0 to prevent warnings for classes with 0 precision/recall\n",
    "\n",
    "print(\"\\nConfusion Matrix (Multi-class, after SMOTE training):\")\n",
    "print(confusion_matrix(y_test, y_pred_smote))\n",
    "\n",
    "# ---  Evaluate the model on the ORIGINAL, UNTOUCHED test set (Binary Malicious vs. Not Malicious) ---\n",
    "print(\"\\n--- Binary (Malicious vs. Not Malicious) Classification Report (after SMOTE training) ---\")\n",
    "\n",
    "# Convert true labels (y_test) to binary (using the original y_test)\n",
    "y_test_binary_smote_eval = np.where(y_test == NORMAL_ENCODED_VALUE, 0, 1)\n",
    "\n",
    "# Convert the NEW predicted labels (y_pred_smote) to binary\n",
    "y_pred_binary_smote_eval = np.where(y_pred_smote == NORMAL_ENCODED_VALUE, 0, 1)\n",
    "\n",
    "print(classification_report(y_test_binary_smote_eval, y_pred_binary_smote_eval, target_names=['Not Malicious (0)', 'Malicious (1)'], zero_division=0))\n",
    "\n",
    "print(\"\\n--- Binary Confusion Matrix (after SMOTE training) ---\")\n",
    "print(confusion_matrix(y_test_binary_smote_eval, y_pred_binary_smote_eval))\n",
    "\n",
    "\n",
    "print(\"\\n--- Investigating FALSE NEGATIVES (Actual Malicious, Predicted Not Malicious) after SMOTE training ---\")\n",
    "\n",
    "false_negatives_indices_smote = np.where((y_test_binary_smote_eval == 1) & (y_pred_binary_smote_eval == 0))[0]\n",
    "\n",
    "if len(false_negatives_indices_smote) > 0:\n",
    "    print(f\"\\nTotal FALSE NEGATIVES (after SMOTE training): {len(false_negatives_indices_smote)}\")\n",
    "\n",
    "    # Get the ACTUAL multi-class labels for these False Negatives\n",
    "    # Use .iloc[] for robust indexing with Pandas Series\n",
    "    actual_labels_for_fns_smote = y_test.iloc[false_negatives_indices_smote]\n",
    "\n",
    "    # Convert encoded actual labels back to human-readable\n",
    "    actual_fn_labels_smote = [encoded_to_label[label] for label in actual_labels_for_fns_smote]\n",
    "\n",
    "    # Analyze the distribution of actual types of missed attacks\n",
    "    missed_attack_counts_smote = pd.Series(actual_fn_labels_smote).value_counts()\n",
    "    print(\"\\nDistribution of Missed Attack Types (False Negatives) after SMOTE training:\")\n",
    "    print(missed_attack_counts_smote)\n",
    "else:\n",
    "    print(\"No FALSE NEGATIVES found after SMOTE training. Model is perfect at not missing attacks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9484da-e122-4a68-9831-45db8e430979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to: C:\\Users\\seena\\Downloads\\Network-Intrusion-Detection-System\\model\\rf_model_smote.pkl\n",
      "Model 'rf_model_smote.pkl' successfully saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Define the directory where models will be saved\n",
    "MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, \"model\")\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"rf_model_smote.pkl\"\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, MODEL_FILENAME)\n",
    "\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# --- Save the Trained Model ---\n",
    "try:\n",
    "    \n",
    "    joblib.dump(rf_model_smote, MODEL_SAVE_PATH)\n",
    "    print(f\"Model '{MODEL_FILENAME}' successfully saved!\")\n",
    "except NameError:\n",
    "    print(\"Error: 'rf_model_smote' is not defined. Please ensure your model training cell was run.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fea631-3197-4e97-ad40-e00bea175c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
